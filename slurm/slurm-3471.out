The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
[32m[I 2023-07-13 14:20:58,917][0m A new study created in memory with name: no-name-53c6ec23-303a-4964-b361-d95df8a4deb9[0m
[[36m2023-07-13 14:20:58,917[0m][[35mHYDRA[0m] Study name: no-name-53c6ec23-303a-4964-b361-d95df8a4deb9[0m
[[36m2023-07-13 14:20:58,917[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2023-07-13 14:20:58,917[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2023-07-13 14:20:58,918[0m][[35mHYDRA[0m] Directions: ['minimize'][0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] Joblib.Parallel(n_jobs=-1,backend=loky,prefer=processes,require=None,verbose=0,timeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=None,mmap_mode=r) is launching 10 jobs[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] Launching jobs, sweep output dir : /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#0 : model.optimizer.lr=0.01923279309285134 data.batch_size=128 model.net.lin1_size=512 model.net.lin2_size=2048 model.net.lin3_size=512 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#1 : model.optimizer.lr=0.03708805040356046 data.batch_size=256 model.net.lin1_size=2048 model.net.lin2_size=512 model.net.lin3_size=512 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#2 : model.optimizer.lr=0.03175192860467024 data.batch_size=64 model.net.lin1_size=512 model.net.lin2_size=1024 model.net.lin3_size=2048 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#3 : model.optimizer.lr=0.004830792352271362 data.batch_size=32 model.net.lin1_size=2048 model.net.lin2_size=1024 model.net.lin3_size=512 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#4 : model.optimizer.lr=0.07907336089239764 data.batch_size=32 model.net.lin1_size=2048 model.net.lin2_size=512 model.net.lin3_size=2048 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#5 : model.optimizer.lr=0.07387845330872035 data.batch_size=32 model.net.lin1_size=2048 model.net.lin2_size=512 model.net.lin3_size=1024 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#6 : model.optimizer.lr=0.07062915675166916 data.batch_size=128 model.net.lin1_size=512 model.net.lin2_size=1024 model.net.lin3_size=512 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,932[0m][[35mHYDRA[0m] 	#7 : model.optimizer.lr=0.005805852245077103 data.batch_size=256 model.net.lin1_size=1024 model.net.lin2_size=1024 model.net.lin3_size=2048 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,933[0m][[35mHYDRA[0m] 	#8 : model.optimizer.lr=0.04583538372485302 data.batch_size=32 model.net.lin1_size=1024 model.net.lin2_size=1024 model.net.lin3_size=2048 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:20:58,933[0m][[35mHYDRA[0m] 	#9 : model.optimizer.lr=0.03358394844902855 data.batch_size=32 model.net.lin1_size=1024 model.net.lin2_size=512 model.net.lin3_size=512 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-13 14:21:11,236[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,242[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,244[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-13 14:21:11,249[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-13 14:21:11,255[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,261[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,263[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-13 14:21:11,268[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 32                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.07907336089239764                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 512                                                        
â”‚         lin3_size: 2048                                                       
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 128                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.01923279309285134                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 512                                                        
â”‚         lin2_size: 2048                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
[rank: 0] Global seed set to 12345
[rank: 0] Global seed set to 12345
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.03175192860467024                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 512                                                        
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 2048                                                       
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 32                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.004830792352271362                                              
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
[[36m2023-07-13 14:21:11,333[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[rank: 0] Global seed set to 12345
[[36m2023-07-13 14:21:11,338[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[rank: 0] Global seed set to 12345
[[36m2023-07-13 14:21:11,354[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 14:21:11,355[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 14:21:11,363[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,365[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,370[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-13 14:21:11,372[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,372[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-13 14:21:11,372[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:21:11,380[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-13 14:21:11,380[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 128                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.07062915675166916                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 512                                                        
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 256                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.005805852245077103                                              
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 1024                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 2048                                                       
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
[rank: 0] Global seed set to 12345
[rank: 0] Global seed set to 12345
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 32                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.07387845330872035                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 512                                                        
â”‚         lin3_size: 1024                                                       
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 256                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.03708805040356046                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 512                                                        
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
[[36m2023-07-13 14:21:11,448[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 14:21:11,450[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[rank: 0] Global seed set to 12345
[rank: 0] Global seed set to 12345
[[36m2023-07-13 14:21:11,470[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 14:21:11,473[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 14:21:12,646[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,648[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,648[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,648[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,648[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,650[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,650[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,653[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:21:12,727[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,727[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,733[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,734[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,735[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,735[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,735[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,736[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,736[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,738[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-13 14:21:12,739[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,741[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,741[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,742[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,742[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,744[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-13 14:21:12,765[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,767[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,772[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,773[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,773[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,774[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,774[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,774[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,774[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,775[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,775[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,778[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-13 14:21:12,778[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,779[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,780[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,780[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,780[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,781[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,781[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,781[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,781[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,782[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,782[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,782[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,784[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-13 14:21:12,784[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,785[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,785[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,785[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-13 14:21:12,786[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,787[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,788[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,788[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,788[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,790[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,790[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-13 14:21:12,790[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,791[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,791[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,792[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-13 14:21:12,800[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:21:12,800[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:21:12,804[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:21:12,805[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:21:12,805[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:21:12,806[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:21:12,806[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:21:12,808[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,709[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,782[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,840[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,859[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,880[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,885[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,892[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:21:13,893[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-07-13 14:21:14,003[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-13 14:21:14,005[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-13 14:21:14,030[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-13 14:21:14,054[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-13 14:21:14,059[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-13 14:21:14,078[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-13 14:21:14,082[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-13 14:21:14,116[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚  4.5 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚  4.5 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  2.2 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  1.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  1.0 M â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 4.5 M                                                         
Non-trainable params: 0                                                         
Total params: 4.5 M                                                             
Total estimated model params size (MB): 17                                      
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚  3.4 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚  3.4 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  2.2 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  525 K â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 3.4 M                                                         
Non-trainable params: 0                                                         
Total params: 3.4 M                                                             
Total estimated model params size (MB): 13                                      
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚  8.1 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚  8.1 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  4.4 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  1.0 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  524 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 8.1 M                                                         
Non-trainable params: 0                                                         
Total params: 8.1 M                                                             
Total estimated model params size (MB): 32                                      
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚  5.4 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚  5.4 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  2.2 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  525 K â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  524 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 10.7 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 10.7 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  1.0 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  525 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  262 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Trainable params: 5.4 M                                                         
Non-trainable params: 0                                                         
Total params: 5.4 M                                                             
Total estimated model params size (MB): 21                                      
Trainable params: 10.7 M                                                        
Non-trainable params: 0                                                         
Total params: 10.7 M                                                            
Total estimated model params size (MB): 42                                      
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.5 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.5 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  1.0 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  1.1 M â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  524 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
SLURM auto-requeueing enabled. Setting signal handlers.
Trainable params: 11.5 M                                                        
Non-trainable params: 0                                                         
Total params: 11.5 M                                                            
Total estimated model params size (MB): 45                                      
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 10.3 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 10.3 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  1.0 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  262 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 10.3 M                                                        
Non-trainable params: 0                                                         
Total params: 10.3 M                                                            
Total estimated model params size (MB): 41                                      
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 192/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 36/36 0:00:00 â€¢       173.81it/s v_num: 0         
                                    0:00:00                    val/loss: 0.053  
                                                               val/mae: 0.14    
                                                               val/mae_best:    
                                                               0.108 train/loss:
                                                               0.016 train/mae: 
                                                               0.085            
[[36m2023-07-13 14:35:48,745[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/6/checkpoints/epoch_092.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/6/checkpoints/epoch_092.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚    0.02925274334847927    â”‚
â”‚         test/mae          â”‚    0.10243266820907593    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0:00:00 â€¢ 0:00:00 294.98it/s 
[[36m2023-07-13 14:35:50,638[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/6/checkpoints/epoch_092.ckpt[0m
[[36m2023-07-13 14:35:50,639[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/6[0m
[[36m2023-07-13 14:35:50,950[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.10847773402929306>[0m
[[36m2023-07-13 14:36:09,856[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:36:10,036[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 32                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.04583538372485302                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 1024                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 2048                                                       
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[rank: 0] Global seed set to 12345
[[36m2023-07-13 14:36:10,386[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 14:36:10,390[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:36:10,447[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:36:10,447[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:36:10,628[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:36:10,630[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:36:10,630[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:36:11,005[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:36:11,006[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:36:11,008[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:36:24,415[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-07-13 14:36:25,319[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚  8.1 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚  8.1 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  4.4 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  1.0 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  524 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 8.1 M                                                         
Non-trainable params: 0                                                         
Total params: 8.1 M                                                             
Total estimated model params size (MB): 32                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 167/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 72/72 0:00:00 â€¢       163.38it/s v_num: 0         
                                    0:00:00                    val/loss: 0.034  
                                                               val/mae: 0.12    
                                                               val/mae_best:    
                                                               0.12 train/loss: 
                                                               0.018 train/mae: 
                                                               0.091            
[[36m2023-07-13 14:37:18,073[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/2/checkpoints/epoch_067.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/2/checkpoints/epoch_067.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚    0.03273836523294449    â”‚
â”‚         test/mae          â”‚    0.11335942894220352    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21/21 0:00:00 â€¢ 0:00:00 447.68it/s 
[[36m2023-07-13 14:37:19,604[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/2/checkpoints/epoch_067.ckpt[0m
[[36m2023-07-13 14:37:19,606[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/2[0m
[[36m2023-07-13 14:37:19,896[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.12006453424692154>[0m
[[36m2023-07-13 14:37:39,834[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 14:37:40,231[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 32                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.03358394844902855                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 1024                                                       
â”‚         lin2_size: 512                                                        
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[rank: 0] Global seed set to 12345
[[36m2023-07-13 14:37:40,623[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 14:37:40,628[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 14:37:40,675[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 14:37:40,676[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 14:37:40,717[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 14:37:40,719[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 14:37:40,720[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 14:37:41,010[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 14:37:41,011[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 14:37:41,014[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 14:37:53,106[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-07-13 14:37:54,303[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚  5.3 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚  5.3 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  4.4 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  524 K â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  262 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 5.3 M                                                         
Non-trainable params: 0                                                         
Total params: 5.3 M                                                             
Total estimated model params size (MB): 21                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 263/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 36/36 0:00:00 â€¢       165.22it/s v_num: 0         
                                    0:00:00                    val/loss: 0.039  
                                                               val/mae: 0.12    
                                                               val/mae_best:    
                                                               0.099 train/loss:
                                                               0.014 train/mae: 
                                                               0.076            
[[36m2023-07-13 14:43:04,861[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/0/checkpoints/epoch_163.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/0/checkpoints/epoch_163.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.027687322348356247    â”‚
â”‚         test/mae          â”‚    0.09490381926298141    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0:00:00 â€¢ 0:00:00 351.92it/s 
[[36m2023-07-13 14:43:07,541[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/0/checkpoints/epoch_163.ckpt[0m
[[36m2023-07-13 14:43:07,542[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/0[0m
[[36m2023-07-13 14:43:07,972[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.09884992241859436>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 183/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 144/144 0:00:01 â€¢       143.38it/s v_num: 0       
                                      0:00:00                    val/loss: 0.102
                                                                 val/mae: 0.215 
                                                                 val/mae_best:  
                                                                 0.164          
                                                                 train/loss:    
                                                                 0.022          
                                                                 train/mae:     
                                                                 0.107          
[[36m2023-07-13 14:48:25,193[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/4/checkpoints/epoch_083.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/4/checkpoints/epoch_083.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚    0.05534644424915314    â”‚
â”‚         test/mae          â”‚    0.15440168976783752    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 41/41 0:00:00 â€¢ 0:00:00 378.81it/s 
[[36m2023-07-13 14:48:26,451[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/4/checkpoints/epoch_083.ckpt[0m
[[36m2023-07-13 14:48:26,453[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/4[0m
[[36m2023-07-13 14:48:26,735[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.16380111873149872>[0m
Epoch 254/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 18/18 0:00:00 â€¢       130.40it/s v_num: 0         
                                    0:00:00                    val/loss: 0.034  
                                                               val/mae: 0.105   
                                                               val/mae_best:    
                                                               0.095 train/loss:
                                                               0.016 train/mae: 
                                                               0.082            
[[36m2023-07-13 14:48:38,807[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/7/checkpoints/epoch_154.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/7/checkpoints/epoch_154.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚    0.02565683424472809    â”‚
â”‚         test/mae          â”‚    0.08968357741832733    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6/6 0:00:00 â€¢ 0:00:00 228.31it/s 
[[36m2023-07-13 14:48:39,978[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/7/checkpoints/epoch_154.ckpt[0m
[[36m2023-07-13 14:48:39,979[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/7[0m
[[36m2023-07-13 14:48:40,188[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.09481623023748398>[0m
slurmstepd-haicluster3: error: *** JOB 3471 ON haicluster3 CANCELLED AT 2023-07-13T14:50:58 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-haicluster3: error: *** STEP 3471.0 ON haicluster3 CANCELLED AT 2023-07-13T14:50:58 DUE TO TIME LIMIT ***
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
srun: got SIGCONT
Epoch 252/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 18/18 0:00:00 â€¢       121.30it/s v_num: 0         
                                    0:00:00                    val/loss: 0.035  
                                                               val/mae: 0.116   
                                                               val/mae_best:    
                                                               0.095 train/loss:
                                                               0.018 train/mae: 
                                                               0.088            
srun: forcing job termination
[[36m2023-07-13 14:50:59,172[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/1[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
Epoch 130/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 144/144 0:00:01 â€¢       141.02it/s v_num: 0       
                                      0:00:00                    val/loss: 0.073
                                                                 val/mae: 0.177 
                                                                 val/mae_best:  
                                                                 0.158          
                                                                 train/loss:    
                                                                 0.021          
                                                                 train/mae:     
                                                                 0.105          
[[36m2023-07-13 14:51:00,972[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/8[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
Epoch 217/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 144/144 0:00:01 â€¢       141.16it/s v_num: 0       
                                      0:00:00                    val/loss: 0.074
                                                                 val/mae: 0.189 
                                                                 val/mae_best:  
                                                                 0.159          
                                                                 train/loss:    
                                                                 0.022          
                                                                 train/mae:     
                                                                 0.108          
Epoch 208/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 144/144 0:00:01 â€¢       140.65it/s v_num: 0       
                                      0:00:00                    val/loss: 0.099
                                                                 val/mae: 0.211 
                                                                 val/mae_best:  
                                                                 0.162          
                                                                 train/loss:    
                                                                 0.02 train/mae:
                                                                 0.099          
Epoch 145/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 144/144 0:00:00 â€¢       221.22it/s v_num: 0       
                                      0:00:00                    val/loss: 0.081
                                                                 val/mae: 0.185 
                                                                 val/mae_best:  
                                                                 0.151          
                                                                 train/loss:    
                                                                 0.021          
                                                                 train/mae:     
                                                                 0.103          
[[36m2023-07-13 14:51:01,417[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/9[0m
[[36m2023-07-13 14:51:01,448[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/5[0m
[[36m2023-07-13 14:51:01,454[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-13_14-20-55/3[0m
srun: error: Timed out waiting for job step to complete
