The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
[32m[I 2023-07-14 08:34:32,066][0m A new study created in memory with name: no-name-5f9fd102-82bb-4502-9ff6-598d29685758[0m
[[36m2023-07-14 08:34:32,066[0m][[35mHYDRA[0m] Study name: no-name-5f9fd102-82bb-4502-9ff6-598d29685758[0m
[[36m2023-07-14 08:34:32,066[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2023-07-14 08:34:32,066[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2023-07-14 08:34:32,067[0m][[35mHYDRA[0m] Directions: ['minimize'][0m
[[36m2023-07-14 08:34:32,075[0m][[35mHYDRA[0m] Joblib.Parallel(n_jobs=-1,backend=loky,prefer=processes,require=None,verbose=0,timeout=None,pre_dispatch=2*n_jobs,batch_size=auto,temp_folder=None,max_nbytes=None,mmap_mode=r) is launching 6 jobs[0m
[[36m2023-07-14 08:34:32,075[0m][[35mHYDRA[0m] Launching jobs, sweep output dir : /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29[0m
[[36m2023-07-14 08:34:32,076[0m][[35mHYDRA[0m] 	#0 : model.optimizer.lr=0.01923279309285134 data.batch_size=128 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-14 08:34:32,076[0m][[35mHYDRA[0m] 	#1 : model.optimizer.lr=0.027332001267735898 data.batch_size=128 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-14 08:34:32,076[0m][[35mHYDRA[0m] 	#2 : model.optimizer.lr=0.03584594526879088 data.batch_size=128 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-14 08:34:32,076[0m][[35mHYDRA[0m] 	#3 : model.optimizer.lr=0.05616349898795594 data.batch_size=256 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-14 08:34:32,076[0m][[35mHYDRA[0m] 	#4 : model.optimizer.lr=0.0365521097917471 data.batch_size=256 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-14 08:34:32,076[0m][[35mHYDRA[0m] 	#5 : model.optimizer.lr=0.06517267650833508 data.batch_size=64 experiment=stars_experiment.yaml trainer=gpu logger=tensorboard hparams_search=stars_optuna.yaml[0m
[[36m2023-07-14 08:34:44,736[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-14 08:34:44,744[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 128                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.03584594526879088                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
[[36m2023-07-14 08:34:44,785[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-14 08:34:44,786[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-14 08:34:44,793[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-14 08:34:44,794[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-14 08:34:44,794[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-14 08:34:44,796[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[rank: 0] Global seed set to 12345
[[36m2023-07-14 08:34:44,803[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-14 08:34:44,804[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
[[36m2023-07-14 08:34:44,822[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-14 08:34:44,835[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-14 08:34:44,847[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.06517267650833508                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 256                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0365521097917471                                                
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 128                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.027332001267735898                                              
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 128                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.01923279309285134                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
[rank: 0] Global seed set to 12345
[rank: 0] Global seed set to 12345
[rank: 0] Global seed set to 12345
[rank: 0] Global seed set to 12345
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 256                                                         
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.05616349898795594                                               
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multir
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multi
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 12345                                                                   
â””â”€â”€ optimized_metric
    â””â”€â”€ val/mae_best                                                            
[[36m2023-07-14 08:34:44,892[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-14 08:34:44,894[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-14 08:34:44,902[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-14 08:34:44,902[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[rank: 0] Global seed set to 12345
[[36m2023-07-14 08:34:44,915[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-14 08:34:46,091[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-14 08:34:46,092[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-14 08:34:46,092[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-14 08:34:46,092[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-14 08:34:46,093[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-14 08:34:46,093[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-14 08:34:46,193[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-14 08:34:46,193[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-14 08:34:46,198[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-14 08:34:46,199[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-14 08:34:46,200[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-14 08:34:46,204[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-14 08:34:46,204[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-14 08:34:46,206[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-14 08:34:46,207[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-14 08:34:46,208[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-14 08:34:46,211[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-14 08:34:46,212[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-14 08:34:46,213[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-14 08:34:46,213[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-14 08:34:46,214[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-14 08:34:46,215[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-14 08:34:46,230[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-14 08:34:46,231[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-14 08:34:46,236[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-14 08:34:46,237[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-14 08:34:46,237[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-14 08:34:46,237[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-14 08:34:46,238[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-14 08:34:46,239[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-14 08:34:46,239[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-14 08:34:46,239[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-14 08:34:46,240[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-14 08:34:46,240[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-14 08:34:46,240[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-14 08:34:46,241[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-14 08:34:46,242[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-14 08:34:46,243[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-14 08:34:46,244[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-14 08:34:46,245[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-14 08:34:46,245[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-14 08:34:46,245[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-14 08:34:46,245[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-14 08:34:46,246[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-14 08:34:46,246[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-14 08:34:46,246[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-14 08:34:46,247[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-14 08:34:46,247[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-14 08:34:46,247[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-14 08:34:46,247[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-14 08:34:46,247[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-14 08:34:46,248[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-14 08:34:46,250[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
[[36m2023-07-14 08:34:46,250[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-14 08:34:47,202[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-14 08:34:47,229[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-14 08:34:47,337[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-14 08:34:47,352[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-14 08:34:47,358[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-14 08:34:47,365[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-07-14 08:34:47,549[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-14 08:34:47,550[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-14 08:34:47,550[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-14 08:34:47,555[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-14 08:34:47,559[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
[[36m2023-07-14 08:34:47,566[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
SLURM auto-requeueing enabled. Setting signal handlers.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ BatchNorm1d       â”‚  4.1 K â”‚
â”‚ 4  â”‚ net.model.2  â”‚ ReLU              â”‚      0 â”‚
â”‚ 5  â”‚ net.model.3  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 6  â”‚ net.model.4  â”‚ BatchNorm1d       â”‚  2.0 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  524 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ BatchNorm1d       â”‚  1.0 K â”‚
â”‚ 10 â”‚ net.model.8  â”‚ ReLU              â”‚      0 â”‚
â”‚ 11 â”‚ net.model.9  â”‚ Linear            â”‚  131 K â”‚
â”‚ 12 â”‚ net.model.10 â”‚ BatchNorm1d       â”‚    512 â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 15 â”‚ net.model.13 â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 16 â”‚ net.model.14 â”‚ ReLU              â”‚      0 â”‚
â”‚ 17 â”‚ net.model.15 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 18 â”‚ net.model.16 â”‚ BatchNorm1d       â”‚     64 â”‚
â”‚ 19 â”‚ net.model.17 â”‚ ReLU              â”‚      0 â”‚
â”‚ 20 â”‚ net.model.18 â”‚ Linear            â”‚    132 â”‚
â”‚ 21 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 22 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 23 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 24 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 25 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 26 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 27 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 28 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 159/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 72/72 0:00:00 â€¢       130.10it/s v_num: 0         
                                    0:00:00                    val/loss: 0.056  
                                                               val/mae: 0.16    
                                                               val/mae_best:    
                                                               0.151 train/loss:
                                                               0.02 train/mae:  
                                                               0.098            
[[36m2023-07-14 08:56:05,712[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/5/checkpoints/epoch_059.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/5/checkpoints/epoch_059.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.046579599380493164    â”‚
â”‚         test/mae          â”‚    0.14421820640563965    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21/21 0:00:00 â€¢ 0:00:00 358.87it/s 
[[36m2023-07-14 08:56:06,503[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/5/checkpoints/epoch_059.ckpt[0m
[[36m2023-07-14 08:56:06,505[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/5[0m
[[36m2023-07-14 08:56:06,671[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.15103943645954132>[0m
Epoch 190/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 36/36 0:00:00 â€¢       104.17it/s v_num: 0         
                                    0:00:00                    val/loss: 0.042  
                                                               val/mae: 0.129   
                                                               val/mae_best:    
                                                               0.107 train/loss:
                                                               0.015 train/mae: 
                                                               0.081            
[[36m2023-07-14 08:59:35,782[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/1/checkpoints/epoch_090.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/1/checkpoints/epoch_090.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.028541410341858864    â”‚
â”‚         test/mae          â”‚    0.09943097084760666    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0:00:00 â€¢ 0:00:00 245.31it/s 
[[36m2023-07-14 08:59:36,574[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/1/checkpoints/epoch_090.ckpt[0m
[[36m2023-07-14 08:59:36,576[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/1[0m
[[36m2023-07-14 08:59:36,705[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.10665453970432281>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 225/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 18/18 0:00:00 â€¢       139.82it/s v_num: 0         
                                    0:00:00                    val/loss: 0.03   
                                                               val/mae: 0.102   
                                                               val/mae_best:    
                                                               0.096 train/loss:
                                                               0.015 train/mae: 
                                                               0.08             
[[36m2023-07-14 09:02:37,816[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/3/checkpoints/epoch_125.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/3/checkpoints/epoch_125.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.027474259957671165    â”‚
â”‚         test/mae          â”‚    0.09125321358442307    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6/6 0:00:00 â€¢ 0:00:00 253.04it/s 
[[36m2023-07-14 09:02:38,488[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/3/checkpoints/epoch_125.ckpt[0m
[[36m2023-07-14 09:02:38,489[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/3[0m
[[36m2023-07-14 09:02:38,497[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.0957183837890625>[0m
Epoch 230/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 36/36 0:00:00 â€¢       132.26it/s v_num: 0         
                                    0:00:00                    val/loss: 0.042  
                                                               val/mae: 0.126   
                                                               val/mae_best:    
                                                               0.111 train/loss:
                                                               0.017 train/mae: 
                                                               0.085            
[[36m2023-07-14 09:02:46,393[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/2/checkpoints/epoch_130.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/2/checkpoints/epoch_130.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.031348008662462234    â”‚
â”‚         test/mae          â”‚    0.10458698868751526    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0:00:00 â€¢ 0:00:00 350.80it/s 
[[36m2023-07-14 09:02:46,638[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/2/checkpoints/epoch_130.ckpt[0m
[[36m2023-07-14 09:02:46,640[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/2[0m
[[36m2023-07-14 09:02:46,647[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.11062765121459961>[0m
Epoch 230/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 36/36 0:00:00 â€¢       161.69it/s v_num: 0         
                                    0:00:00                    val/loss: 0.039  
                                                               val/mae: 0.123   
                                                               val/mae_best:    
                                                               0.107 train/loss:
                                                               0.015 train/mae: 
                                                               0.08             
[[36m2023-07-14 09:02:51,860[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (36) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/0/checkpoints/epoch_130.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/0/checkpoints/epoch_130.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.029255645349621773    â”‚
â”‚         test/mae          â”‚    0.10097485035657883    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0:00:00 â€¢ 0:00:00 364.11it/s 
[[36m2023-07-14 09:02:52,104[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/0/checkpoints/epoch_130.ckpt[0m
[[36m2023-07-14 09:02:52,106[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/0[0m
[[36m2023-07-14 09:02:52,119[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.10699297487735748>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
`Trainer.fit` stopped: `max_epochs=400` reached.
Epoch 399/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 18/18 0:00:00 â€¢       139.49it/s v_num: 0         
                                    0:00:00                    val/loss: 0.036  
                                                               val/mae: 0.119   
                                                               val/mae_best:    
                                                               0.103 train/loss:
                                                               0.018 train/mae: 
                                                               0.088            
[[36m2023-07-14 09:07:25,466[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/4/checkpoints/epoch_347.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/4/checkpoints/epoch_347.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.028712334111332893    â”‚
â”‚         test/mae          â”‚    0.09791587293148041    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6/6 0:00:00 â€¢ 0:00:00 264.43it/s 
[[36m2023-07-14 09:07:25,681[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/4/checkpoints/epoch_347.ckpt[0m
[[36m2023-07-14 09:07:25,682[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/multiruns/2023-07-14_08-34-29/4[0m
[[36m2023-07-14 09:07:25,689[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <val/mae_best=0.1033167615532875>[0m
[[36m2023-07-14 09:07:25,704[0m][[35mHYDRA[0m] Best parameters: {'model.optimizer.lr': 0.05616349898795594, 'data.batch_size': 256}[0m
[[36m2023-07-14 09:07:25,705[0m][[35mHYDRA[0m] Best value: 0.0957183837890625[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
