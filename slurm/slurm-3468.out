The activation script must be sourced, otherwise the virtual environment will not work.
Setting vars
[[36m2023-07-13 13:19:49,522[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-07-13 13:19:49,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.stars_datamodule.StarsDataModule                     
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       batch_size: 64                                                          
â”‚       train_val_test_split:                                                   
â”‚       - 0.7                                                                   
â”‚       - 0.1                                                                   
â”‚       - 0.2                                                                   
â”‚       num_workers: 0                                                          
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.stars_module.StarsLitModule                        
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       net:                                                                    
â”‚         _target_: src.models.components.multilayer_perceptron.MLP             
â”‚         input_size: 4302                                                      
â”‚         lin1_size: 2048                                                       
â”‚         lin2_size: 1024                                                       
â”‚         lin3_size: 512                                                        
â”‚         lin4_size: 256                                                        
â”‚         lin5_size: 64                                                         
â”‚         lin6_size: 32                                                         
â”‚         output_size: 4                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         dirpath: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/runs/2
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/mae                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: lightning.pytorch.callbacks.EarlyStopping                   
â”‚         monitor: val/mae                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: lightning.pytorch.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: lightning.pytorch.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ tensorboard:                                                            
â”‚         _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger     
â”‚         save_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/runs/
â”‚         name: stars                                                           
â”‚         log_graph: false                                                      
â”‚         default_hp_metric: true                                               
â”‚         prefix: ''                                                            
â”‚       aim:                                                                    
â”‚         experiment: stars                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.pytorch.trainer.Trainer                             
â”‚       default_root_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train
â”‚       min_epochs: 100                                                         
â”‚       max_epochs: 400                                                         
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚       gradient_clip_val: 0.5                                                  
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /p/haicluster/niesel1/lightning-hydra-stars                   
â”‚       data_dir: /p/haicluster/niesel1/lightning-hydra-stars/data/             
â”‚       log_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/              
â”‚       output_dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/runs/
â”‚       work_dir: /p/haicluster/niesel1/lightning-hydra-stars/src               
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['stars', 'mlp']                                                        
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ compile
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ 12345                                                                   
[rank: 0] Global seed set to 12345
[[36m2023-07-13 13:19:49,603[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.stars_datamodule.StarsDataModule>[0m
[[36m2023-07-13 13:19:50,477[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.stars_module.StarsLitModule>[0m
[[36m2023-07-13 13:19:50,593[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-07-13 13:19:50,594[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>[0m
[[36m2023-07-13 13:19:50,600[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>[0m
[[36m2023-07-13 13:19:50,601[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>[0m
[[36m2023-07-13 13:19:50,602[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>[0m
[[36m2023-07-13 13:19:50,602[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-07-13 13:19:50,603[0m][[34msrc.utils.instantiators[0m][[32mINFO[0m] - Instantiating logger <lightning.pytorch.loggers.tensorboard.TensorBoardLogger>[0m
[[36m2023-07-13 13:19:50,604[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <lightning.pytorch.trainer.Trainer>[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:196: UserWarning: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-07-13 13:19:51,465[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-07-13 13:19:51,738[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name         â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ net          â”‚ MLP               â”‚ 11.6 M â”‚
â”‚ 1  â”‚ net.model    â”‚ Sequential        â”‚ 11.6 M â”‚
â”‚ 2  â”‚ net.model.0  â”‚ Linear            â”‚  8.8 M â”‚
â”‚ 3  â”‚ net.model.1  â”‚ ReLU              â”‚      0 â”‚
â”‚ 4  â”‚ net.model.2  â”‚ Linear            â”‚  2.1 M â”‚
â”‚ 5  â”‚ net.model.3  â”‚ ReLU              â”‚      0 â”‚
â”‚ 6  â”‚ net.model.4  â”‚ Linear            â”‚  524 K â”‚
â”‚ 7  â”‚ net.model.5  â”‚ ReLU              â”‚      0 â”‚
â”‚ 8  â”‚ net.model.6  â”‚ Linear            â”‚  131 K â”‚
â”‚ 9  â”‚ net.model.7  â”‚ ReLU              â”‚      0 â”‚
â”‚ 10 â”‚ net.model.8  â”‚ Linear            â”‚ 16.4 K â”‚
â”‚ 11 â”‚ net.model.9  â”‚ ReLU              â”‚      0 â”‚
â”‚ 12 â”‚ net.model.10 â”‚ Linear            â”‚  2.1 K â”‚
â”‚ 13 â”‚ net.model.11 â”‚ ReLU              â”‚      0 â”‚
â”‚ 14 â”‚ net.model.12 â”‚ Linear            â”‚    132 â”‚
â”‚ 15 â”‚ criterion    â”‚ MSELoss           â”‚      0 â”‚
â”‚ 16 â”‚ train_mae    â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 17 â”‚ val_mae      â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 18 â”‚ test_mae     â”‚ MeanAbsoluteError â”‚      0 â”‚
â”‚ 19 â”‚ train_loss   â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 20 â”‚ val_loss     â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 21 â”‚ test_loss    â”‚ MeanMetric        â”‚      0 â”‚
â”‚ 22 â”‚ val_mae_best â”‚ MinMetric         â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 11.6 M                                                        
Non-trainable params: 0                                                         
Total params: 11.6 M                                                            
Total estimated model params size (MB): 46                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 233/399 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 72/72 0:00:00 â€¢       269.08it/s v_num: 0         
                                    0:00:00                    val/loss: 0.007  
                                                               val/mae: 0.038   
                                                               val/mae_best:    
                                                               0.038 train/loss:
                                                               0.004 train/mae: 
                                                               0.029            
[[36m2023-07-13 13:27:51,119[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Restoring states from the checkpoint path at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/runs/2023-07-13_13-19-49/checkpoints/epoch_133.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]
Loaded model weights from the checkpoint at /p/haicluster/niesel1/lightning-hydra-stars/logs/train/runs/2023-07-13_13-19-49/checkpoints/epoch_133.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚   0.005747918505221605    â”‚
â”‚         test/mae          â”‚    0.03601397946476936    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21/21 0:00:00 â€¢ 0:00:00 437.21it/s 
[[36m2023-07-13 13:27:51,376[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/runs/2023-07-13_13-19-49/checkpoints/epoch_133.ckpt[0m
[[36m2023-07-13 13:27:51,377[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /p/haicluster/niesel1/lightning-hydra-stars/logs/train/runs/2023-07-13_13-19-49[0m
[[36m2023-07-13 13:27:51,384[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Metric name is None! Skipping metric value retrieval...[0m
/p/haicluster/niesel1/venv_stars/venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
